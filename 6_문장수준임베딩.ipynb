{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6.문장수준임베딩.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1qZEMIyt_j7TrGXojncI5xpkz3vIU5Uk5",
      "authorship_tag": "ABX9TyPQmrUmXstgBvGP6EOTjY/E"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0i-yeTD4xnd"
      },
      "source": [
        "# 문장수준 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-jAj1kdzRsQ"
      },
      "source": [
        "root_path = \"/content/drive/MyDrive/2021-1/AI데이터활용교재개발/code\"\n",
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfo2o5UuwwCX"
      },
      "source": [
        "## LDA : 잠재의미분석\n",
        "잠재의미분석(LSA : Latent semantic analysis)\n",
        "\n",
        "단어-문서 행렬이나 TF-IDF 행렬에 특이값 분해로 차원축소를 시행하고 여기세어 해당하는 벡터를 취해 임베딩을 만드는 방법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T2tD0Jtwn2B"
      },
      "source": [
        "# mecab 설치\n",
        "! pip install konlpy\n",
        "\n",
        "# google colab에서 mecab을 사용하기 편하게 만들어주는 shell 파일도 실행시켜 줍니다.\n",
        "! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "! bash ./Mecab-ko-for-Google-Colab/install_mecab-ko_on_colab_light_210108.sh\n",
        "\n",
        "# shell 파일 출처: https://somjang.tistory.com/entry/Google-Colab에서-Mecab-koMecab-ko-dic-쉽게-사용하기 [솜씨좋은장씨]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7ZcWZffzU8K"
      },
      "source": [
        "import konlpy\n",
        "from konlpy.tag import Mecab\n",
        "import pandas as pd\n",
        "\n",
        "output_fname = root_path + \"/data/processed/processed_movie2.txt\"\n",
        "# corpus_fname = root_path + \"/data/raw/NewsResult.xlsx\"\n",
        "corpus_fname = root_path + \"/data/processed/processed_review_movieid.txt\"\n",
        "model_path = root_path + \"/model/lsa-tfidf2.vecs\"\n",
        "tokenizer = Mecab()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K5GAtN9RlYr"
      },
      "source": [
        "# 영화댓글 전처리\n",
        "noun_corpus = []\n",
        "titles = []\n",
        "raw_corpus= []\n",
        "with open(corpus_fname, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                sentence, movie_id = line.strip().split(\"\\u241E\")\n",
        "                raw_corpus.append(sentence)\n",
        "                titles.append(movie_id)\n",
        "                tokens = tokenizer.nouns(sentence)\n",
        "                noun_corpus.append(' '.join(tokens))\n",
        "            except:\n",
        "                continue\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8DmhHQ03CUO"
      },
      "source": [
        "# corpus = pd.read_excel(corpus_fname)\n",
        "\n",
        "titles = corpus['제목'].str.replace(\"\\n\",\" \")\n",
        "raw_corpus = corpus['본문'].str.replace(\"\\n\",\" \")\n",
        "noun_corpus = []\n",
        "\n",
        "for text in raw_corpus:\n",
        "    nouns = tokenizer.nouns(text)\n",
        "    noun_corpus.append(' '.join(nouns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFHhk0kQzQ8u"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "# construct tf-idf matrix\n",
        "vectorizer = TfidfVectorizer(\n",
        "    min_df=1,\n",
        "    ngram_range=(1, 1),\n",
        "    lowercase=True,\n",
        "    tokenizer=lambda x: x.split())\n",
        "input_matrix = vectorizer.fit_transform(noun_corpus)\n",
        "\n",
        "#\n",
        "# compute truncated SVD(Singular Value Decomposition)\n",
        "svd = TruncatedSVD(n_components=100)\n",
        "vecs = svd.fit_transform(input_matrix)\n",
        "with open(model_path, 'w') as f:\n",
        "    for doc_idx, vec in enumerate(vecs):\n",
        "        str_vec = [str(el) for el in vec]\n",
        "        f.writelines(titles[doc_idx] + \"\\u241E\" + raw_corpus[doc_idx] + '\\u241E' + ' '.join(str_vec) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eec4CMsi4qVg"
      },
      "source": [
        "# LSA수행\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "class LSAEvaluator:\n",
        "    def __init__(self, model_fname=\"data/sentence-embeddings/lsa-tfidf/lsa-tfidf.vecs\",\n",
        "                 use_notebook=True):\n",
        "        self.titles, self.vectors = self.load_model(model_fname)\n",
        "        self.use_notebook = use_notebook\n",
        "\n",
        "    def most_similar(self, doc_id, topn=10):\n",
        "        query_doc_vec = self.vectors[doc_id]\n",
        "        query_vec_norm = np.linalg.norm(query_doc_vec)\n",
        "        if query_vec_norm != 0:\n",
        "            query_unit_vec = query_doc_vec / query_vec_norm\n",
        "        else:\n",
        "            query_unit_vec = query_doc_vec\n",
        "        query_sentence = self.titles[doc_id]\n",
        "        scores = np.dot(self.vectors, query_unit_vec)\n",
        "        return [query_sentence, sorted(zip(self.titles, scores), key=lambda x: x[1], reverse=True)[1:topn + 1]]\n",
        "\n",
        "    def load_model(self, model_fname):\n",
        "        titles, vectors = [], []\n",
        "\n",
        "        with open(model_fname, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                title, _, str_vec = line.strip().split(\"\\u241E\")\n",
        "                vector = [float(el) for el in str_vec.split()]\n",
        "                titles.append(title)\n",
        "                vectors.append(vector)\n",
        "        return titles, normalize(vectors, axis=1, norm='l2')\n",
        "\n",
        "\n",
        "model = LSAEvaluator(model_path, use_notebook = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9un6Cfl3RNV9"
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "titles, vectors = [], []\n",
        "\n",
        "with open(model_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        title, _, str_vec = line.strip().split(\"\\u241E\")\n",
        "        vector = [float(el) for el in str_vec.split()]\n",
        "        titles.append(title)\n",
        "        vectors.append(vector)\n",
        "\n",
        "vectors =  normalize(vectors, axis=1, norm='l2')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU2DgEKdRuKF"
      },
      "source": [
        "# 벡터 내적을 통해 가장 비슷한 영화를 찾아주는 코드\n",
        "import requests\n",
        "from lxml import html\n",
        "def most_similar(doc_id, vectors, titles, topn=10):\n",
        "        query_doc_vec = vectors[doc_id]\n",
        "        query_vec_norm = np.linalg.norm(query_doc_vec)\n",
        "        if query_vec_norm != 0:\n",
        "            query_unit_vec = query_doc_vec / query_vec_norm\n",
        "        else:\n",
        "            query_unit_vec = query_doc_vec\n",
        "        query_sentence = titles[doc_id]\n",
        "        scores = np.dot(vectors, query_unit_vec)\n",
        "        return [query_sentence, sorted(zip(titles, scores), key=lambda x: x[1], reverse=True)[1:topn + 1]]\n",
        "\n",
        "\n",
        "def get_movie_title(movie_id):\n",
        "    url = 'http://movie.naver.com/movie/point/af/list.nhn?st=mcode&target=after&sword=%s' % movie_id.split(\"_\")[0]\n",
        "    resp = requests.get(url)\n",
        "    root = html.fromstring(resp.text)\n",
        "    try:\n",
        "        title = root.xpath('//div[@class=\"choice_movie_info\"]//h5//a/text()')[0]\n",
        "    except:\n",
        "        title = \"\"\n",
        "    return title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE5_5ott6bKb"
      },
      "source": [
        "LSA 모델에 대해 타겟문서와 유사문서 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxsVyyym5kAj",
        "outputId": "dd5596aa-ec20-4d3b-f70d-e68ecea58d3d"
      },
      "source": [
        "movie_list = most_similar(doc_id =1, vectors = vectors, titles = titles, topn = 10)\n",
        "\n",
        "print(\"타겟 문서 : \" + get_movie_title(movie_list[0]))\n",
        "\n",
        "print(\"\")\n",
        "for m in movie_list[1]:\n",
        "    print(\"유사 문서 : \" + get_movie_title(m[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "타겟 문서 : 은밀하게 위대하게\n",
            "\n",
            "유사 문서 : 장미의 이름\n",
            "유사 문서 : 카이지\n",
            "유사 문서 : 싸이코\n",
            "유사 문서 : 포세이돈 어드벤쳐\n",
            "유사 문서 : 나는 나를 파괴할 권리가 있다\n",
            "유사 문서 : 둠\n",
            "유사 문서 : 각시탈 철면객\n",
            "유사 문서 : 좋은 놈, 나쁜 놈, 이상한 놈\n",
            "유사 문서 : 쏘우 5\n",
            "유사 문서 : 크라잉 프리맨\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvQxEx7B_hDS"
      },
      "source": [
        "## Doc2Vec\n",
        "- w2v에 이어 구글팀이 개발한 문서 임베딩 기법\n",
        "- 문서를 하나의 word처럼 취급해서 임베딩한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c_tJxYZ_lhV"
      },
      "source": [
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from gensim.models import Doc2Vec, ldamulticore\n",
        "\n",
        "def make_form(corpus_fname):\n",
        "    tagged = []\n",
        "\n",
        "    tokenizer = Mecab()\n",
        "    with open(corpus_fname, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                sentence, movie_id = line.strip().split(\"\\u241E\")\n",
        "                tokens = tokenizer.morphs(sentence)\n",
        "                tagged_doc = TaggedDocument(words=tokens, tags=['MOVIE_%s' % movie_id])\n",
        "                tagged.append(tagged_doc)\n",
        "            except:\n",
        "                continue\n",
        "    return tagged\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_form2(corpus_fname):\n",
        "    corpus = pd.read_excel(corpus_fname)\n",
        "    titles = corpus['제목'].str.replace(\"\\n\",\" \")\n",
        "    raw_corpus = corpus['본문'].str.replace(\"\\n\",\" \")\n",
        "\n",
        "    tokenizer = Mecab()\n",
        "    tagged = []\n",
        "    for i,title in enumerate( titles):\n",
        "        tokens = tokenizer.morphs(title)\n",
        "        tagged_doc = TaggedDocument(words=tokens, tags=['NEWS_%s' % str(i)])\n",
        "        tagged.append(tagged_doc)\n",
        "\n",
        "    return tagged\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAP2kYkaOvcd"
      },
      "source": [
        "output_fname = root_path + \"/data/processed/processed_movie_d2v.txt\"\n",
        "corpus_fname = root_path + \"/data/processed/processed_review_movieid.txt\"\n",
        "\n",
        "corpus = make_form(corpus_fname)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nI2A2oAi-Ck"
      },
      "source": [
        "corpus_fname = root_path + \"/data/raw/NewsResult.xlsx\"\n",
        "\n",
        "output_fname = root_path + \"/model/lda_news\"\n",
        "\n",
        "corpus = make_form2(corpus_fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg4UWyOXaULd"
      },
      "source": [
        "model = Doc2Vec(corpus, vector_size=50)                                                \n",
        "model.save(output_fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm7M_ReEIvyw"
      },
      "source": [
        "from lxml import html\n",
        "import sys, requests, random\n",
        "\n",
        "class Doc2VecEvaluator:\n",
        "    def __init__(self, model_fname=\"data/doc2vec.vecs\", use_notebook=False):\n",
        "        self.model = Doc2Vec.load(model_fname)\n",
        "        self.doc2idx = {el:idx for idx, el in enumerate(self.model.docvecs.doctags.keys())}\n",
        "        self.use_notebook = use_notebook\n",
        "\n",
        "    def most_similar(self, movie_id, topn=10):\n",
        "        similar_movies = self.model.docvecs.most_similar('MOVIE_' + str(movie_id), topn=topn)\n",
        "        for movie_id, score in similar_movies:\n",
        "            print(self.get_movie_title(movie_id), score)\n",
        "\n",
        "    def get_titles_in_corpus(self, n_sample=5):\n",
        "        movie_ids = random.sample(self.model.docvecs.doctags.keys(), n_sample)\n",
        "        return {movie_id: self.get_movie_title(movie_id) for movie_id in movie_ids}\n",
        "\n",
        "    def get_movie_title(self, movie_id):\n",
        "        url = 'http://movie.naver.com/movie/point/af/list.nhn?st=mcode&target=after&sword=%s' % movie_id.split(\"_\")[1]\n",
        "        resp = requests.get(url)\n",
        "        root = html.fromstring(resp.text)\n",
        "        try:\n",
        "            title = root.xpath('//div[@class=\"choice_movie_info\"]//h5//a/text()')[0]\n",
        "        except:\n",
        "            title = \"\"\n",
        "        return title\n",
        "\n",
        "    def visualize_movies(self, n_sample=30, palette=\"Viridis256\", type=\"between\"):\n",
        "        movie_ids = self.get_titles_in_corpus(n_sample=n_sample)\n",
        "        movie_titles = [movie_ids[key] for key in movie_ids.keys()]\n",
        "        movie_vecs = [self.model.docvecs[self.doc2idx[movie_id]] for movie_id in movie_ids.keys()]\n",
        "        if type == \"between\":\n",
        "            visualize_between_words(movie_titles, movie_vecs, palette, use_notebook=self.use_notebook)\n",
        "        else:\n",
        "            visualize_words(movie_titles, movie_vecs, palette, use_notebook=self.use_notebook)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovqWscTpkIRk"
      },
      "source": [
        "# News\n",
        "from lxml import html\n",
        "import sys, requests, random\n",
        "\n",
        "class Doc2VecEvaluator2:\n",
        "    def __init__(self, corpus_fname, model_fname=\"data/doc2vec.vecs\", use_notebook=False):\n",
        "        corpus = pd.read_excel(corpus_fname)\n",
        "        self.model = Doc2Vec.load(model_fname)\n",
        "        self.doc2idx = {el:idx for idx, el in enumerate(self.model.docvecs.doctags.keys())}\n",
        "        self.use_notebook = use_notebook\n",
        "        self.titles =  corpus['제목'].str.replace(\"\\n\",\" \")\n",
        "\n",
        "    def most_similar(self, movie_id, topn=10):\n",
        "        similar_movies = self.model.docvecs.most_similar('NEWS_' + str(movie_id), topn=topn)\n",
        "        print(\"타겟 문서 : \" + self.titles[int(movie_id)])\n",
        "        print(\"\")\n",
        "        for movie_id, score in similar_movies:\n",
        "            print(\"유사문서 : \" + self.titles[int(movie_id[5:])] +\" \"+ str(score))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwnRNp_k6z5I"
      },
      "source": [
        "뉴스제목 비슷한 것 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP67SfsXkTVi"
      },
      "source": [
        "model = Doc2VecEvaluator2(root_path + \"/data/raw/NewsResult.xlsx\",output_fname)\n",
        "model.most_similar(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1axZcnsQ6vqG"
      },
      "source": [
        "영화에 대해 유사한 영화 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fmUM5K-kVg5",
        "outputId": "0dfc2460-8677-4844-9a8b-c3359f7d7169"
      },
      "source": [
        "model = Doc2VecEvaluator(output_fname)\n",
        "model.get_titles_in_corpus(n_sample = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MOVIE_10851': '특경도룡'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbgcxk-7aGoo",
        "outputId": "02ade8f8-6342-4f7c-d875-33288f04f8ee"
      },
      "source": [
        "# 92575 : '은밀하게 위대하게'\n",
        "model.most_similar(92575, topn = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "고스톱 살인 0.7861570119857788\n",
            "롤플레이 0.7839573621749878\n",
            "이니셜 D - 극장판 0.7826319932937622\n",
            "맥스 페인 0.7758529782295227\n",
            "짐승 0.7750623822212219\n",
            "칠검 0.7738490104675293\n",
            "내일의 죠 0.7731249332427979\n",
            "48 + 1 0.772041916847229\n",
            "아이리스 - 극장판 0.7693309783935547\n",
            "베를린 0.7651496529579163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeOk_z8weubp"
      },
      "source": [
        "model.doc2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtmWKkJH3mQF"
      },
      "source": [
        "## LDA : Latent Dirichlet Allocation\n",
        "\n",
        "- 주어진 문서에 대하여 각 문서에 어떤 토픽들이 존재하는지에 대한 확률 모델\n",
        "- 말뭉치 이면에 잠재된 토픽(주제)를 추출한다는 의미에서 토필 모델링이라고 부르기도 한다.\n",
        "- 토픽의 수를 미리 정해야 한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-bT7dOHgX5M"
      },
      "source": [
        "from gensim.models import ldamulticore\n",
        "from gensim import corpora\n",
        "import pandas as pd\n",
        "import pdb\n",
        "\n",
        "# corpus_fname = root_path + \"/data/raw/NewsResult.xlsx\"\n",
        "corpus_fname = root_path + \"/data/processed/processed_review_movieid.txt\"\n",
        "output_fname = root_path + '/model/lda'\n",
        "\n",
        "document, tokenized_corpus = [], []\n",
        "\n",
        "with open(corpus_fname, 'r', encoding='utf-8') as f:\n",
        "    \n",
        "    for sentence in f:\n",
        "        tokens = list(set(tokenizer.nouns(sentence.strip())))\n",
        "        document.append(sentence)\n",
        "        tokenized_corpus.append(tokens)\n",
        "dictionary = corpora.Dictionary(tokenized_corpus)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_corpus]\n",
        "LDA = ldamulticore.LdaMulticore(corpus, id2word=dictionary,\n",
        "                                num_topics=30,\n",
        "                                minimum_probability=0.0,\n",
        "                                workers=4)\n",
        "# 특정 토픽의 확률이 0.5보다 클 경우에만 데이터를 리턴한다\n",
        "# 확률의 합은 1이기 때문에 해당 토픽이 해당 문서에서 확률값이 가장 큰 토픽이 된다\n",
        "all_topics = LDA.get_document_topics(corpus, minimum_probability=0.5, per_word_topics=False)\n",
        "with open(output_fname + \".resultss\", 'w') as f:\n",
        "    for doc_idx, topic in enumerate(all_topics):\n",
        "        if len(topic) == 1:\n",
        "            topic_id, prob = topic[0]\n",
        "            f.writelines(document[doc_idx].strip() + \"\\u241E\" + ' '.join(tokenized_corpus[doc_idx]) + \"\\u241E\" + str(topic_id) + \"\\u241E\" + str(prob) + \"\\n\")\n",
        "LDA.save(output_fname + \".model\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d36b0e1CG-V"
      },
      "source": [
        "from collections import defaultdict\n",
        "from gensim.models import LdaModel\n",
        "class LDAEvaluator:\n",
        "\n",
        "    def __init__(self, model_path, tokenizera):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.all_topics = self.load_results(model_path + \".results\")\n",
        "        self.model = LdaModel.load(model_path + \".model\")\n",
        "\n",
        "    def load_results(self, results_fname):\n",
        "        topic_dict = defaultdict(list)\n",
        "        with open(results_fname, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                sentence, _, topic_id, prob = line.strip().split(\"\\u241E\")\n",
        "                topic_dict[int(topic_id)].append((sentence, float(prob)))\n",
        "        for key in topic_dict.keys():\n",
        "            topic_dict[key] = sorted(topic_dict[key], key=lambda x: x[1], reverse=True)\n",
        "        return topic_dict\n",
        "\n",
        "    def show_topic_docs(self, topic_id, topn=10):\n",
        "        return self.all_topics[topic_id][:topn]\n",
        "\n",
        "    def show_topic_words(self, topic_id, topn=10):\n",
        "        return self.model.show_topic(topic_id, topn=topn)\n",
        "\n",
        "    def show_new_document_topic(self, documents):\n",
        "        tokenized_documents = [self.tokenizer.nouns(document) for document in documents]\n",
        "        curr_corpus = [self.model.id2word.doc2bow(tokenized_document) for tokenized_document in tokenized_documents]\n",
        "        topics = self.model.get_document_topics(curr_corpus, minimum_probability=0.5, per_word_topics=False)\n",
        "        for doc_idx, topic in enumerate(topics):\n",
        "            if len(topic) == 1:\n",
        "                topic_id, prob = topic[0]\n",
        "                print(documents[doc_idx], \", topic id:\", str(topic_id), \", prob:\", str(prob))\n",
        "            else:\n",
        "                print(documents[doc_idx], \", there is no dominant topic\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQtDeRyFCO49"
      },
      "source": [
        "model = LDAEvaluator(output_fname,tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBKVz7vQCZgC",
        "outputId": "a5b31321-15bb-41f9-f1ba-aedd2abfd4d9"
      },
      "source": [
        "# 토픽 10 에 있는 댓글들\n",
        "text = model.show_topic_docs(topic_id =10)\n",
        "for t in text:\n",
        "    print(t)\n",
        "for t in text:\n",
        "    print(tokenizer.nouns(t[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('10년전에도 10년이 지난 지금에도 못보겠는건 마찬가지...다들 그 풋풋했던 그 시절 느낌이 좋았다는데 난 그때 감성이 없었나봄..', 0.9731481)\n",
            "('구라안치고 알바아니고 마블팬도아닌데 정말 나는 재밌게 봤음.8~9점주고싶지만 평점조절을 위해 10점', 0.9697917)\n",
            "('4년만에 다시 보게된 킬미는 그때보다 깊이있고 더 좋다..강혜정의 대사 한마디 한마디가 어찌나 콕콕 박히는지..', 0.96666664)\n",
            "('10점을 기준으로 평가했던 다른 영화들의 점수를 내릴 수가 없어서 이것만 12점 정도 주고싶은 느낌', 0.96666664)\n",
            "('왜 별점이 이러냐 알바생들인가 현실을 뒤덮으려하지마라 4대강은 잘못된정책이다.', 0.9641975)\n",
            "('다들 알바들인가? 왜이렇게 점수가후해.. 0점주고싶은데 안되서 1점줍니다~~', 0.9628205)\n",
            "('남자배우 제레미 아이언스 인줄 알았음..노숙자가 단숨에 집도마련하고,미국이좆쿠나', 0.9597222)\n",
            "('생각보다 볼만하고 평점이 너무 낮아 10점줌 냉정하게 7.8점 정도 영화임...', 0.9597222)\n",
            "('평점이 너무 낮은거 아닌가... 나름 괜찬던데 긴장감도 있고 적어도 8점짜리 영화라고 생각함', 0.9597222)\n",
            "('서 울고 알바생들이 단체로와서 10점 찍었네..ㅋㅋ 솔직히 3점도 아깝다.', 0.9597222)\n",
            "['년', '전', '년', '지금', '건', '마찬가지', '다', '시절', '느낌', '데', '그때', '감성']\n",
            "['구라', '바', '마블', '팬', '나', '점', '평점', '조절', '점']\n",
            "['년', '만', '킬미', '그때', '강혜정', '대사', '한마디', '한마디']\n",
            "['점', '기준', '평가', '영화', '점수', '수', '이것', '점', '정도', '느낌']\n",
            "['별점', '바', '현실', '대강', '정책']\n",
            "['바', '점수', '후', '점', '점']\n",
            "['남자', '배우', '제레미', '아이언스', '인줄', '노숙', '집', '마련', '미국']\n",
            "['생각', '볼', '평점', '점', '줌', '냉정', '점', '정도', '영화']\n",
            "['평점', '거', '나름', '긴장감', '점', '영화', '생각']\n",
            "['바', '단체', '점', '점']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED2pQMusFj_n",
        "outputId": "f79f226c-35d7-4112-fe9f-8e3d422ffb3a"
      },
      "source": [
        "# 토픽들\n",
        "model.show_topic_words(topic_id = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('말', 0.100373015),\n",
              " ('액션', 0.09122683),\n",
              " ('편', 0.06642185),\n",
              " ('영화', 0.04693405),\n",
              " ('필요', 0.046399146),\n",
              " ('스릴러', 0.029394941),\n",
              " ('수작', 0.02696253),\n",
              " ('물', 0.024509482),\n",
              " ('걸작', 0.01568005),\n",
              " ('맛', 0.014427378)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9bsx1CFFq6s",
        "outputId": "6660a004-f973-47cd-cde0-f479d0d4a434"
      },
      "source": [
        "model.show_new_document_topic([\"말이 필요없는 액션 영화. 스릴러물\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "말이 필요없는 액션 영화. 스릴러물 , topic id: 10 , prob: 0.6424094\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}